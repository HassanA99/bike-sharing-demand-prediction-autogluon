# Report: Predict Bike Sharing Demand with AutoGluon Solution

#### Hassan Wurie Jalloh

## Initial Training
### What did you realize when you tried to submit your predictions? What changes were needed to the output of the predictor to submit your results?

When I first submitted the predictions generated by AutoGluon, I realized Kaggle might reject the file due to negative prediction values. Since we are forecasting bike rentals, it doesn't also make practical sense to have negative counts and Kaggle enforces this strictly.

To resolve this, I inspected the prediction distribution using .describe() and (preds < 0).sum() to identify any negative values. I then applied .clip(lower=0) to ensure all predictions were non-negative before formatting the final submission.csv. This step was crucial to create a valid Kaggle submission.

### What was the top ranked model that performed?

The best-performing model, according to AutoGluon's validation leaderboard, was:

    WeightedEnsemble_L3 with an RMSE (score_val) of 56.59.

    This model outperformed all base learners, including:

    LightGBM_BAG_L2 (RMSE: 57.22)

    LightGBMXT_BAG_L2 (RMSE: 60.76)

    KNN, RandomForest, CatBoost, and others (RMSE > 84+)

This result demonstrates how powerful AutoGluon's multi-level stacking is. The ensemble at level 3 effectively combines multiple strong predictors, yielding the best generalization performance on unseen data.

## Exploratory data analysis and feature creation
### What did the exploratory analysis find and how did you add additional features?

During the exploratory data analysis (EDA), I found several meaningful patterns related to time-based trends and weather conditions that affect bike rental demand:

Hourly patterns: Demand was noticeably higher during the morning (7–9 AM), lunch (11 AM–1 PM), and evening rush (5–7 PM).

Weekday vs. weekend: There were behavioral differences in demand between weekdays and weekends.

Seasonal impact: Certain seasons showed significantly different usage trends, with summer months peaking.

Weather and temperature: Clear weather and mild temperatures generally led to higher demand.
##### Feature Engineering
To better capture these effects, I engineered several new features:

    hour, day, month, weekday – extracted from the datetime column.

    rush_hour – categorizes time as morning, lunch, evening, or off_peak.

###### Binned features:

    temp_bin: cold, mild, hot

    humidity_bin: dry, normal, humid

    windspeed_bin: calm, breezy, windy

    Converted features to categorical for better modeling by tree-based algorithms:

    season, weather, rush_hour, binned variables

    These new features were designed to help models understand latent groupings and nonlinear relationships in the data.



### How much better did your model preform after adding additional features and why do you think that is?

After feature engineering, the model’s validation RMSE improved from 56.59 to 30.57, as shown in the AutoGluon leaderboard:

| Model                      | RMSE (score\_val) |
| -------------------------- | ----------------- |
| Before feature engineering | **56.59**         |
| After feature engineering  | **30.57**         |

This substantial performance boost is likely due to:

Better representation of time-based demand cycles

Simplified patterns for weather and temperature through binning

Enhanced modeling by leveraging categorical encodings

Overall, these features helped the ensemble model generalize much better to unseen data.


## Hyper parameter tuning
### How much better did your model preform after trying different hyper parameters?

After completing multiple training runs using best_quality presets, I introduced randomized hyperparameter tuning using hyperparameter_tune_kwargs with num_trials=20. The model selected during this tuned run was LightGBMLarge_BAG_L1, further stacked into WeightedEnsemble_L3, achieving a validation RMSE of 33.85.

| Experiment                  | Best Model                              | RMSE (Validation) |
| --------------------------- | --------------------------------------- | ----------------- |
| Before HPO                  | `WeightedEnsemble_L3` (default presets) | **56.59**         |
| After Feature Engineering   | `WeightedEnsemble_L3`                   | **30.57**         |
| After Hyperparameter Tuning | `WeightedEnsemble_L3`                   | **33.85**         |

Interestingly, the tuned model's RMSE was slightly worse than the feature-engineered model without HPO. This can happen due to:

Overfitting on validation splits during HPO

Reduced model diversity in the tuning trial set

Smaller search space or fewer trials (we only ran 20 trials)

In this case, the gain from high-quality features was stronger than additional tuning. Still, the HPO exercise provided useful insights and confirmed that feature engineering is a top priority in tabular datasets like this.

### If you were given more time with this dataset, where do you think you would spend more time?

If I had more time, I would focus on:

Stacked Feature Engineering: Introduce advanced lag features or time-window aggregates (e.g., rolling hourly averages, historical 7-day mean rentals).

Hybrid Modeling: Combine time series models like Prophet with AutoGluon’s tabular predictions to blend trend + event-driven behavior.

Outlier Detection and Treatment: Manually clean potential outliers in windspeed, temperature, or holiday anomalies which may skew model training.

Model-Specific Tuning: Use Bayesian optimization or Optuna across specific model families (e.g., tune LightGBM depth, learning rate, number of leaves).

Deployment Pipeline: Wrap the trained AutoGluon model in a scalable endpoint (e.g., FastAPI) for real-time forecasting.


### Create a table with the models you ran, the hyperparameters modified, and the kaggle score.
|model|hpo1|hpo2|hpo3|score|
|--|--|--|--|--|
|initial|None|None|None|1.76767|
|add_features|None|None|None|0.65867|
|hpo|20|"random"|"default"|0.50470|



### Create a line plot showing the top model score for the three (or more) training runs during the project.

TODO: Replace the image below with your own.

![model_train_score.png](img/model_train_score.png)

### Create a line plot showing the top kaggle score for the three (or more) prediction submissions during the project.

TODO: Replace the image below with your own.

![model_test_score.png](img/model_test_score.png)

## Summary

This project successfully demonstrated the application of AutoGluon for predicting bike sharing demand. Initial training established a baseline RMSE of 56.59. The most significant improvement came from comprehensive feature engineering, which reduced the RMSE to 30.57 by extracting temporal and weather-related insights. While hyperparameter tuning (HPO) did not further improve the validation RMSE, it still yielded a strong result of 33.85 and provided valuable insights into the model's robustness. The final Kaggle submission scores reflect these improvements, with the HPO run achieving the best score of 0.50470. Future work could involve more advanced feature engineering, hybrid modeling approaches, and detailed outlier analysis to further enhance predictive accuracy.
